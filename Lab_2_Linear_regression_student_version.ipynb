{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MvsSKLO7vIoF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix45IT1SXgE-"
      },
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start working on a real dataset\n",
        "\n",
        "We're going to use the House Price dataset we used last time ([link text](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data?select=train.csv))\n",
        "\n",
        "Try to upload the dataset on your Google Drive and access it through Colab."
      ],
      "metadata": {
        "id": "3c9D97gpwNcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the file on your system and copy and paste the path here\n",
        "FILE_PATH = \"/content/train.csv\"\n",
        "df = ..."
      ],
      "metadata": {
        "id": "dx5x1wW-spG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show all the columns\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "_eTFI7PIBhyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the shape of the dataset\n"
      ],
      "metadata": {
        "id": "hy8tBC-espDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data types\n"
      ],
      "metadata": {
        "id": "4qiwS91CaY8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the first few records\n"
      ],
      "metadata": {
        "id": "040j4XxLBVVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the columns\n"
      ],
      "metadata": {
        "id": "HbEUkBN54GXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the correlation of columns with .corr()\n",
        "\n",
        "# plot the correlations with sns.heatmap\n"
      ],
      "metadata": {
        "id": "527Ed0yVpCvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the code bellow to have a better view of corrolations\n",
        "def plot_corr_matrix(df, nr_c, targ) :\n",
        "    \"\"\"\n",
        "    A function for getting the features with the highest\n",
        "    corrolation with the target variable.\n",
        "    \"\"\"\n",
        "\n",
        "    # get the values\n",
        "    corr = df.corr()\n",
        "    corr_abs = corr.abs()\n",
        "\n",
        "    # get the names in the largest to smallest order\n",
        "    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n",
        "    cm = np.corrcoef(df[cols].values.T)\n",
        "\n",
        "    # plot the figure\n",
        "    plt.figure(figsize=(nr_c/1.5, nr_c/1.5))\n",
        "    sns.set(font_scale=1.25)\n",
        "    sns.heatmap(cm, linewidths=1.5, annot=True, square=True,\n",
        "                fmt='.2f', annot_kws={'size': 10},\n",
        "                yticklabels=cols.values, xticklabels=cols.values)\n",
        "    print('The highest corrolations are with the following columns:\\n', cols)\n",
        "    plt.show()\n",
        "\n",
        "plot_corr_matrix(df, nr_c=5, targ='SalePrice')"
      ],
      "metadata": {
        "id": "5tF_DPMtb8eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the list of columns printed out on the output of previous code cell\n",
        "# and paste it here\n",
        "columns_to_use =\n",
        "\n",
        "# filter the above columns of the dataset and save the new dataset into df_sample\n",
        "df_sample = ..."
      ],
      "metadata": {
        "id": "K9GyxJ_rA5Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look into the number of nulls we have in df_sample\n"
      ],
      "metadata": {
        "id": "8d8Q12SpspB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the sampled dataset\n"
      ],
      "metadata": {
        "id": "zzUFLZREo2Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot GrLivArea against SalePrice with a scatter plot\n"
      ],
      "metadata": {
        "id": "wLQhRK6NrOWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the outliers (ones that have GrLivArea more than 4000)\n",
        "df_sample = ...\n",
        "\n",
        "# plot it again\n"
      ],
      "metadata": {
        "id": "JDnNXnTL0S4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable called X_columns and put all the column names\n",
        "# (except for your target column name) in it\n",
        "x_columns= ...\n",
        "\n",
        "print(x_columns)\n",
        "\n",
        "# filter df_sample based on the x_column_names and your target name\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "# build the train and test sets\n",
        "X_train, X_test, y_train, y_test = ..."
      ],
      "metadata": {
        "id": "byASczRn_wwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries for regression\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "metadata": {
        "id": "xYmMRbbO12Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a linear regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kyXbarrTD2cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Ridge regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L21z96KP3Eej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Lasso regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "ze2-GQOx3D2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "col = 'GrLivArea'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Add a reference line for a perfect prediction (y_test = predictions)\n",
        "\n",
        "\n",
        "# Create a scatter plot for Linear Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Ridge Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Lasso Regression\n",
        "\n",
        "\n",
        "\n",
        "# Set plot labels and title\n",
        "\n",
        "\n",
        "# Show the plot\n"
      ],
      "metadata": {
        "id": "509g_vnf3tPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* How are weights calculated: [link](https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843)\n",
        "\n",
        "* Why is Lasso harder on coefficients in comparison to Ridge? [link](https://online.stat.psu.edu/stat508/lesson/5/5.4)\n",
        "\n",
        "* Regression requirements: [link](https://www.youtube.com/watch?v=0MFpOQRY0rw&ab_channel=zedstatistics)\n",
        "\n",
        "* Linear regression in sklearn: [link](https://scikit-learn.org/stable/modules/linear_model.html#linear-models)\n",
        "\n",
        "* Lasso Regression\n"
      ],
      "metadata": {
        "id": "e8z2eHZFJEYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add and convert categorical columns to numerical"
      ],
      "metadata": {
        "id": "7n1zefxlEH9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the dataset based on the following columns\n",
        "columns_to_use = ['LotArea', 'YrSold', 'GarageArea', 'GarageYrBlt',\n",
        "                  'GrLivArea', 'OverallQual', 'ExterQual', 'YearBuilt',\n",
        "                  'MSZoning', 'KitchenQual',\n",
        "                  'SalePrice']\n",
        "\n",
        "# save the new dataset into df_sample\n"
      ],
      "metadata": {
        "id": "2IODAssMaDgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the dataset\n"
      ],
      "metadata": {
        "id": "WscABI_XcTsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look into the number of nulls we have\n",
        "# (sort the values to have the emptiest column on top)\n"
      ],
      "metadata": {
        "id": "tMoSmt5Ia6AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fillna with mean for: GarageYrBlt\n"
      ],
      "metadata": {
        "id": "CtC1LF3ja6W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if nulls have been filled"
      ],
      "metadata": {
        "id": "eOrnA7lyf_Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create a second dataset from the first dataset\n",
        "df_transformed = df_sample.copy()\n",
        "\n",
        "# define the categorical columns\n",
        "cols = ('KitchenQual', 'ExterQual', 'MSZoning')\n",
        "\n",
        "# process columns, apply LabelEncoder to categorical features\n",
        "for c in cols:\n",
        "    lbl = LabelEncoder()\n",
        "    lbl.fit(list(df_transformed[c].values))\n",
        "    df_transformed[c] = lbl.transform(list(df_transformed[c].values))\n",
        "\n"
      ],
      "metadata": {
        "id": "newbpFjcbEGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the transformed version\n",
        "# is everything looking good?\n"
      ],
      "metadata": {
        "id": "V0YKZFJfsaF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's go back to the original dataset \"df_sample\" by saving it in df_transformed to try another type of encoding\n"
      ],
      "metadata": {
        "id": "6bmT_zqJtOKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use ordinal encoder to transform kitchen quality (KitchenQual)\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "order = ['Fa','TA','Gd','Ex']\n",
        "columns_with_order = ['ExterQual', 'KitchenQual']\n"
      ],
      "metadata": {
        "id": "w3lWido0tKlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the kitchQual in df_sample\n"
      ],
      "metadata": {
        "id": "ltlbnebFdz1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the kitchQual in df_transformed\n"
      ],
      "metadata": {
        "id": "CSgOR-RZsc3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the columns to be one-hot encoded (MSZoning)\n",
        "# Perform one-hot encoding\n",
        "\n",
        "\n",
        "# Concatenate the encoded columns with the original dataset\n",
        "\n",
        "\n",
        "# Display the modified dataset\n"
      ],
      "metadata": {
        "id": "7NHOzxOdvT4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the columns you transformed and keep the converted versions\n",
        "df_transformed.drop(columns=['MSZoning'], inplace=True)"
      ],
      "metadata": {
        "id": "N_FRORLpejN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the variables we want to use for X (everything except for 'SalePrice')\n",
        "X_columns = ...\n",
        "\n",
        "# create X and y\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "# build the train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "c3T013134uk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View X_train. Is everything looking good?\n",
        "X_train"
      ],
      "metadata": {
        "id": "9U879-iiaANI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a linear regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "La2P9gcEfjFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Ridge regression model\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "Jf6DPAc-fk1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Lasso regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "\n"
      ],
      "metadata": {
        "id": "1jAICYpWfUd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oDaQ1ZrgfrlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "col = 'GrLivArea'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Add a reference line for a perfect prediction (y_test = predictions)\n",
        "\n",
        "\n",
        "# Create a scatter plot for Linear Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Ridge Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Lasso Regression\n",
        "\n",
        "\n",
        "# Set plot labels and title\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "2mk2MlgA40Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NxZtMI4433r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional for now"
      ],
      "metadata": {
        "id": "x6fI6wd6s_to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What if we didn't have normalized features?\n"
      ],
      "metadata": {
        "id": "MvsSKLO7vIoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean and standard deviation for each feature\n",
        "mean = np.mean(X, axis=0)\n",
        "std_dev = np.std(X, axis=0)\n",
        "\n",
        "# Normalize the features using z-score standardization\n",
        "X_normalized = (X - mean) / std_dev\n",
        "\n",
        "# Now, X_normalized contains your normalized features\n",
        "X_normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLOCiqRpvIct",
        "outputId": "303edbe0-6b84-4bbe-b868-4a354827a42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.11880088,  1.30830738, -1.94596119, ..., -0.0799718 ,\n",
              "         0.52392458, -0.2068604 ],\n",
              "       [-0.74303607,  0.68323931, -0.04619011, ..., -0.41725697,\n",
              "        -0.11423462, -0.12904578],\n",
              "       [ 1.47897114, -0.81555906,  0.25820241, ..., -1.04435196,\n",
              "        -0.46699142,  1.01243711],\n",
              "       ...,\n",
              "       [-0.30806122,  0.70698175, -0.10586411, ...,  2.57735983,\n",
              "        -0.41305098,  0.86882684],\n",
              "       [ 0.65574781,  0.677178  , -1.52848625, ..., -0.17924173,\n",
              "         0.02779055, -2.05940375],\n",
              "       [ 0.91263067, -0.54004781,  0.39728375, ...,  1.91455488,\n",
              "        -1.85248203, -0.67698021]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  other ways of doing this\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming you have a numpy array called 'X' containing your features\n",
        "# X.shape should be (number_of_samples, number_of_features)\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to your data and transform the features\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# X_normalized contains your z-score standardized features\n",
        "X_normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5srEpGZvIaQ",
        "outputId": "cc7bfeae-0632-4ee6-d227-c7655e0075d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.11880088,  1.30830738, -1.94596119, ..., -0.0799718 ,\n",
              "         0.52392458, -0.2068604 ],\n",
              "       [-0.74303607,  0.68323931, -0.04619011, ..., -0.41725697,\n",
              "        -0.11423462, -0.12904578],\n",
              "       [ 1.47897114, -0.81555906,  0.25820241, ..., -1.04435196,\n",
              "        -0.46699142,  1.01243711],\n",
              "       ...,\n",
              "       [-0.30806122,  0.70698175, -0.10586411, ...,  2.57735983,\n",
              "        -0.41305098,  0.86882684],\n",
              "       [ 0.65574781,  0.677178  , -1.52848625, ..., -0.17924173,\n",
              "         0.02779055, -2.05940375],\n",
              "       [ 0.91263067, -0.54004781,  0.39728375, ...,  1.91455488,\n",
              "        -1.85248203, -0.67698021]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have a numpy array called 'X' containing your features\n",
        "# X.shape should be (number_of_samples, number_of_features)\n",
        "\n",
        "# Create a MinMaxScaler instance (by default, scales to [0, 1])\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to your data and transform the features\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# X_normalized contains your min-max scaled features\n",
        "X_normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zafK71EvrHO",
        "outputId": "05dc9684-aae1-4b61-a7f5-3903ab0ea6e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.53513317, 0.70448302, 0.19237754, ..., 0.45693682, 0.58597695,\n",
              "        0.48918306],\n",
              "       [0.4381887 , 0.60450883, 0.47924949, ..., 0.39572753, 0.47958645,\n",
              "        0.50160903],\n",
              "       [0.78326906, 0.3647891 , 0.52521381, ..., 0.28192459, 0.42077672,\n",
              "        0.68388883],\n",
              "       ...,\n",
              "       [0.50574081, 0.60830623, 0.47023851, ..., 0.93917979, 0.42976938,\n",
              "        0.66095616],\n",
              "       [0.6554215 , 0.60353938, 0.25541769, ..., 0.43892167, 0.50326413,\n",
              "        0.19335625],\n",
              "       [0.69531571, 0.40885472, 0.54621557, ..., 0.81889634, 0.1897951 ,\n",
              "        0.41411111]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgFllKwNwbX5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}